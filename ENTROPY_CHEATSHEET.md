# Entropy Visual Cheat Sheet

## The Formula in One Line

```
H(p) = -âˆ‘ p(x) Ã— log p(x)
       â†‘  â†‘      â†‘
       â”‚  â”‚      â””â”€ How "surprising" is x?
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€ How likely is x?
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sum over all outcomes, negate at end
```

## The Three-Step Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INPUT: Logits [2.0, 1.0, 0.5, 0.3]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Logits â†’ Log-Probabilities                   â”‚
â”‚                                                       â”‚
â”‚   logsumexp = 2.57                                   â”‚
â”‚   log_probs = [2.0, 1.0, 0.5, 0.3] - 2.57           â”‚
â”‚             = [-0.57, -1.57, -2.07, -2.27]           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Log-Probabilities â†’ Probabilities            â”‚
â”‚                                                       â”‚
â”‚   probs = exp(log_probs)                             â”‚
â”‚         = [0.565, 0.208, 0.126, 0.103]               â”‚
â”‚         (sum = 1.002 â‰ˆ 1.0 âœ“)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Compute Entropy                              â”‚
â”‚                                                       â”‚
â”‚   probs Ã— log_probs = [-0.322, -0.327, -0.261, -0.234]â”‚
â”‚   sum = -1.144                                       â”‚
â”‚   entropy = -(-1.144) = 1.144                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT: Entropy = 1.144                              â”‚
â”‚ (82.5% of max entropy for 4 tokens)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Entropy Spectrum

```
    0.0                      0.5                      1.0                     1.39
     â”‚â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”‚â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”‚â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”‚
     â†‘                        â†‘                        â†‘                       â†‘
  CERTAIN              FAIRLY CERTAIN            UNCERTAIN           COMPLETELY RANDOM
  
  [0.99, 0.01, 0.00, 0.00]    [0.70, 0.20, 0.07, 0.03]    [0.40, 0.30, 0.20, 0.10]    [0.25, 0.25, 0.25, 0.25]
  
  "It's definitely 'A'"       "Probably 'A'"              "Maybe 'A' or 'B'?"         "No clue!"
```

## Quick Reference Table

| Scenario | Probabilities | Entropy | Meaning |
|----------|--------------|---------|---------|
| **Completely certain** | [1.0, 0.0, 0.0, 0.0] | 0.0 | "100% sure" |
| **Very confident** | [0.9, 0.05, 0.03, 0.02] | 0.47 | "Pretty sure" |
| **Somewhat confident** | [0.6, 0.2, 0.1, 0.1] | 1.18 | "Leaning toward..." |
| **Uncertain** | [0.4, 0.3, 0.2, 0.1] | 1.28 | "Could be several" |
| **Completely random** | [0.25, 0.25, 0.25, 0.25] | 1.39 | "No idea!" |

*Note: For vocab_size=V, max entropy = log(V)*

## Code Pattern

```python
# Pattern you'll see everywhere in ML code:

# Convert raw outputs to log-probabilities (stable)
log_probs = logits - torch.logsumexp(logits, dim=-1, keepdim=True)

# Get probabilities
probs = torch.exp(log_probs)

# Compute entropy
entropy = -torch.sum(probs * log_probs, dim=-1)
```

## Common Values for Language Models

```
Vocabulary size: 50,000 tokens
Maximum possible entropy: log(50,000) â‰ˆ 10.82

Typical values during generation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario       â”‚ Entropy  â”‚ What's happening?       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Very confident â”‚  0.5-2.0 â”‚ "I know what comes next"â”‚
â”‚ Normal text    â”‚  2.0-5.0 â”‚ "Typical uncertainty"   â”‚
â”‚ Confused       â”‚  5.0-8.0 â”‚ "Many options possible" â”‚
â”‚ Random/broken  â”‚  8.0+    â”‚ "Model is lost"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Temperature and Entropy

```python
# Temperature controls entropy:

# Low temperature (0.5) â†’ Low entropy
logits_cold = logits / 0.5  # Amplify differences
probs = [0.95, 0.03, 0.01, 0.01]  # Sharp distribution
entropy â‰ˆ 0.3  # Very certain

# Normal temperature (1.0)
logits_normal = logits / 1.0
probs = [0.70, 0.15, 0.10, 0.05]  # Moderate distribution
entropy â‰ˆ 1.0  # Moderately uncertain

# High temperature (2.0) â†’ High entropy
logits_hot = logits / 2.0  # Flatten differences
probs = [0.40, 0.30, 0.20, 0.10]  # Flat distribution
entropy â‰ˆ 1.3  # Very uncertain
```

## When to Use Entropy

### âœ… **Use entropy to:**
- Monitor training progress
- Detect overfitting (entropy too low on training set)
- Measure model confidence
- Implement exploration in RL
- Debug weird model behavior

### âŒ **Don't use entropy for:**
- Measuring model accuracy (use cross-entropy loss instead)
- Selecting the best token (use argmax or sampling instead)
- Comparing different models (use perplexity instead)

## The Intuition

Think of entropy as:
- **A thermometer** for model confidence
- **High** = hot = random = uncertain = "model is guessing"
- **Low** = cold = deterministic = certain = "model is sure"

## Why Log-Sum-Exp?

```python
# The problem with naive approach:
exp(1000) = OVERFLOW!  # Infinity
exp(-1000) = UNDERFLOW!  # Zero

# Log-sum-exp trick:
logsumexp([1000, 999, 998]) = 1000 + log(1 + e^(-1) + e^(-2))
                             = 1000 + log(1 + 0.368 + 0.135)
                             = 1000 + 0.411
                             = 1000.411  # No overflow!
```

## Memory Aid

**ENTROPY = UNCERTAINTY**

- ğŸ“Œ **E**xpected surprise
- ğŸ“Œ **N**ot knowing what's next
- ğŸ“Œ **T**otal randomness measure
- ğŸ“Œ **R**andomness quantified
- ğŸ“Œ **O**utcome unpredictability
- ğŸ“Œ **P**robability spread measure
- ğŸ“Œ **Y**ou don't know!

The more spread out the probabilities, the higher the entropy!
